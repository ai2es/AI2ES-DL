{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "45615978-6adc-4fc3-9f15-3cddbebb5c5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-21 11:53:19.461608: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from itertools import product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a87eb93-41b6-4eff-bee8-c7ae6a7d6b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "class JobIterator():\n",
    "    \"\"\"\n",
    "    JobIterator object used to define the order of an array of experiments. By Andrew H. Fagg,\n",
    "    modified by Jay Rothenberger\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, params):\n",
    "        \"\"\"\n",
    "        Constructor\n",
    "\n",
    "        @param params Dictionary of key/list pairs\n",
    "        \"\"\"\n",
    "        self.params = params\n",
    "        # List of all combinations of parameter values\n",
    "        self.product = list(dict(zip(params, x)) for x in product(*params.values()))\n",
    "        # Iterator over the combinations\n",
    "        self.iter = (dict(zip(params, x)) for x in product(*params.values()))\n",
    "\n",
    "    def next(self):\n",
    "        \"\"\"\n",
    "        @return The next combination in the list\n",
    "        \"\"\"\n",
    "        return self.iter.next()\n",
    "\n",
    "    def get_index(self, i):\n",
    "        \"\"\"\n",
    "        Return the ith combination of parameters\n",
    "\n",
    "        @param i Index into the Cartesian product list\n",
    "        @return The ith combination of parameters\n",
    "        \"\"\"\n",
    "\n",
    "        return self.product[i]\n",
    "\n",
    "    def get_njobs(self):\n",
    "        \"\"\"\n",
    "        @return The total number of combinations\n",
    "        \"\"\"\n",
    "\n",
    "        return len(self.product)\n",
    "\n",
    "    def set_attributes_by_index(self, i, obj):\n",
    "        \"\"\"\n",
    "        For an arbitrary object, set the attributes to match the ith job parameters\n",
    "\n",
    "        @param i Index into the Cartesian product list\n",
    "        @param obj Arbitrary object (to be modified)\n",
    "        @return A string representing the combinations of parameters, and the args object\n",
    "        \"\"\"\n",
    "\n",
    "        # Fetch the ith combination of parameter values\n",
    "        d = self.get_index(i)\n",
    "        # Iterate over the parameters\n",
    "        for k, v in d.items():\n",
    "            obj[k] = v\n",
    "\n",
    "        return obj, self.get_param_str(i)\n",
    "\n",
    "    def get_param_str(self, i):\n",
    "        \"\"\"\n",
    "        Return the string that describes the ith job parameters.\n",
    "        Useful for generating file names\n",
    "\n",
    "        @param i Index into the Cartesian product list\n",
    "        \"\"\"\n",
    "\n",
    "        out = 'JI_'\n",
    "        # Fetch the ith combination of parameter values\n",
    "        d = self.get_index(i)\n",
    "        # Iterate over the parameters\n",
    "        for k, v in d.items():\n",
    "            out = out + \"%s_%s_\" % (k, v)\n",
    "\n",
    "        # Return all but the last character\n",
    "        return out[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ce54897-fdd4-4310-bc54-a7f34933ddbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4386\n"
     ]
    }
   ],
   "source": [
    "# load the data from the pickle file provided by Alberto\n",
    "with open('./giga_df_pickle', 'rb') as fp:\n",
    "    giga_df = pickle.load(fp)\n",
    "# how many examples do we have?\n",
    "print(len(giga_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "31eb2019-53e1-46ca-8e06-e2e74fb8466f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of raw data (rows) 4386\n",
      "features per row: 41\n"
     ]
    }
   ],
   "source": [
    "# dropping columns that contain only nan values\n",
    "#for i in range(len(giga_df)):\n",
    "#    giga_df[i] = giga_df[i][:29] + giga_df[i][39:]\n",
    "# how verifying we still have the same number of examples (sanity check)\n",
    "print(\"size of raw data (rows)\", len(giga_df))\n",
    "print(\"features per row:\", len(giga_df[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8d35a2d3-c86a-467e-8241-1eed63a10a18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of examples: 4386\n",
      "after dropping nan rows 0\n",
      "Empty DataFrame\n",
      "Columns: [4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40]\n",
      "Index: []\n",
      "\n",
      "[0 rows x 35 columns]\n",
      "Empty DataFrame\n",
      "Columns: [29]\n",
      "Index: []\n",
      "Empty DataFrame\n",
      "Columns: [30]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "# convert the list of tuples to a pandas datframe\n",
    "df = pd.DataFrame(giga_df)\n",
    "# drop rows with nan values -  empirically if there are some nans all are nans, so this is sensible\n",
    "nan_rows = df.loc[df.isna().any(axis=1)]\n",
    "# how many rows were in the df?\n",
    "print(\"number of examples:\", len(df))\n",
    "# how many after we dropped the nan rows?\n",
    "print(\"after dropping nan rows\", len(df.dropna()))\n",
    "# drop them\n",
    "df = df.dropna()\n",
    "# filepaths dataframe for image training\n",
    "filepaths = pd.DataFrame(df[29])\n",
    "# labels dataframe\n",
    "labels = pd.DataFrame(df[30])\n",
    "# drop filepath and label columns from features dataframe\n",
    "df = df.drop([29, 30], axis=1)\n",
    "df = df.drop([0, 1, 2, 3], axis=1)\n",
    "# let's take a look at the tabular data\n",
    "print(df.head())\n",
    "print(filepaths.head())\n",
    "print(labels.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1d77d900-ba0e-4b07-8954-cbf4730b8505",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fagg/miniconda3/envs/tf_bleeding5/lib/python3.10/site-packages/sklearn/preprocessing/_encoders.py:808: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0    1    2\n",
       "0  1.0  0.0  0.0\n",
       "1  1.0  0.0  0.0\n",
       "2  1.0  0.0  0.0\n",
       "3  1.0  0.0  0.0\n",
       "4  1.0  0.0  0.0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "# one-hot-encode the string data\n",
    "def encode_string_tokens(df):\n",
    "    # columns that were encoded\n",
    "    redundant_cols = []\n",
    "    for column in df.columns:\n",
    "        # if the column is string-valued\n",
    "        if isinstance(np.unique(df[column])[0], str):\n",
    "            redundant_cols.append(column)\n",
    "            enc = OneHotEncoder(sparse=False)\n",
    "            # one-hot encode it\n",
    "            encoding = enc.fit_transform(np.array(df[column]).reshape(-1, 1))\n",
    "            # create a new binary feature for each position in the encoding\n",
    "            for i in range(len(encoding[0])):\n",
    "                df[f'encoded - {column}.{i}'] = encoding[:, i]\n",
    "            # let's see the unique values\n",
    "            print(np.unique(encoding))\n",
    "    # drop the redundant columns\n",
    "    return df.drop(redundant_cols, axis=1)\n",
    "\n",
    "df = encode_string_tokens(df)\n",
    "df.head()\n",
    "\n",
    "enc = OneHotEncoder(sparse=False)\n",
    "print(type(labels))\n",
    "labels = pd.DataFrame(enc.fit_transform(np.array(labels[labels.columns[0]]).reshape(-1, 1)))\n",
    "\n",
    "labels.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a615df21-efe6-458a-b93a-121783303d58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train size: 2762, val_size: 390, test_size: 794\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Intel(R) Extension for Scikit-learn* enabled (https://github.com/intel/scikit-learn-intelex)\n"
     ]
    }
   ],
   "source": [
    "# Turn on scikit-learn optimizations with these 2 simple lines:\n",
    "\n",
    "from sklearnex import patch_sklearn\n",
    "\n",
    "patch_sklearn()\n",
    "\n",
    "# Import scikit-learn algorithms after the patch is enabled \n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# convert data to numpy\n",
    "X, Y = df.to_numpy(), labels.to_numpy()\n",
    "# shuffle the data\n",
    "np.random.seed(42)\n",
    "np.random.shuffle(X)\n",
    "np.random.shuffle(Y)\n",
    "# split the data\n",
    "x_train, x_val_test, y_train, y_val_test = train_test_split(X, Y, train_size=.7)\n",
    "x_val, x_test, y_val, y_test = train_test_split(x_val_test, y_val_test, train_size=.33)\n",
    "print(f'train size: {len(y_train)}, val_size: {len(y_val)}, test_size: {len(y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0761c874-c163-449b-900e-1c1ed3e6c747",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({'criterion': 'entropy', 'min_samples_leaf': 150, 'max_features': 'log2'}, 0.5820512820512821)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "decision_tree_params = {\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'min_samples_leaf': [10, 20, 50, 100, 150],\n",
    "    'max_features': ['sqrt', 'log2']\n",
    "}\n",
    "\n",
    "ji = JobIterator((decision_tree_params))\n",
    "\n",
    "dt_combination_scores = [] # tuples of hparams, scores\n",
    "\n",
    "for i in range(ji.get_njobs()):\n",
    "    # create a basic decision tree\n",
    "    tree_classifier = DecisionTreeClassifier(**ji.get_index(i))\n",
    "    # fit it\n",
    "    tree_classifier.fit(x_train, y_train)\n",
    "    # evaluate on validation data\n",
    "    dt_combination_scores.append((ji.get_index(i), tree_classifier.score(x_val, y_val)))\n",
    "\n",
    "print(max(dt_combination_scores, key=lambda k: k[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5ca54ba2-b65d-4735-9d8f-f9bcaa30dadb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({'criterion': 'gini', 'min_samples_leaf': 50, 'max_features': 'sqrt'}, 0.6153846153846154)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf_combination_scores = [] # tuples of hparams, scores\n",
    "\n",
    "for i in range(ji.get_njobs()):\n",
    "    # create a basic random forest\n",
    "    forest_classifier = RandomForestClassifier(**ji.get_index(i))\n",
    "    # fit it\n",
    "    forest_classifier.fit(x_train, y_train)\n",
    "    # evaluate it on validation data\n",
    "    \n",
    "    rf_combination_scores.append((ji.get_index(i), forest_classifier.score(x_val, y_val)))\n",
    "\n",
    "print(max(rf_combination_scores, key=lambda k: k[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "965d16ce-d1b0-4caf-b54e-c65575c64514",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.layers import *\n",
    "\n",
    "def MLP(layers, n_classes=3, learning_rate=1e-3, activation='relu'):\n",
    "    inputs = Input((25, ))\n",
    "    x = inputs\n",
    "    x = BatchNormalization()(x)\n",
    "    for units in layers:\n",
    "        x = Dense(units, activation=activation)(x)\n",
    "    outputs = Dense(n_classes, activation='softmax')(x)\n",
    "    \n",
    "    model = tf.keras.models.Model(inputs=[inputs], outputs=outputs)\n",
    "    # you can ignore these next two statements\n",
    "    opt = tf.keras.optimizers.Nadam(learning_rate=learning_rate,\n",
    "                                    beta_1=0.9, beta_2=0.999,\n",
    "                                    epsilon=None, decay=0.99)\n",
    "    opt = tf.keras.mixed_precision.LossScaleOptimizer(opt)\n",
    "    # select the correct kind of accuracy\n",
    "    # compile the model\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=opt,\n",
    "                  metrics=[tf.keras.metrics.CategoricalAccuracy()])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9a747e69-c6c5-4b01-b02f-915f554963ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n",
      "(<keras.engine.functional.Functional object at 0x2b4c5bab1270>, {'layers': [], 'n_classes': 3, 'learning_rate': 0.001, 'activation': 'elu'}, 0.6153846153846154)\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 27.0149 - categorical_accuracy: 0.2985\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[27.01494598388672, 0.29848867654800415]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_params = {\n",
    "    'layers': [[], [32], [3, 3, 3], [10, 12], [32, 16, 8]],\n",
    "    'n_classes': [y_train.shape[-1]],\n",
    "    'learning_rate': [1e-3, 1e-2],\n",
    "    'activation': ['elu']\n",
    "}\n",
    "\n",
    "ji_nn = JobIterator(model_params)\n",
    "\n",
    "nn_combination_scores = [] # tuples of hparams, scores\n",
    "\n",
    "for i in range(ji_nn.get_njobs()):\n",
    "    print(i, end='\\r')\n",
    "    # create a basic random forest\n",
    "    model = MLP(**ji_nn.get_index(i))\n",
    "    # fit it\n",
    "    model.fit(x_train, y_train, validation_data=(x_val, y_val), epochs=100, callbacks=[tf.keras.callbacks.EarlyStopping(monitor='val_categorical_accuracy', patience=5, restore_best_weights=True)], batch_size=32, verbose=0)\n",
    "    # evaluate it on validation data\n",
    "    \n",
    "    nn_combination_scores.append((model, ji_nn.get_index(i), forest_classifier.score(x_val, y_val)))\n",
    "print()\n",
    "print(max(nn_combination_scores, key=lambda k: k[-1]))\n",
    "\n",
    "max(nn_combination_scores, key=lambda k: k[-1])[0].evaluate(x_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d83334-4820-4af1-9534-e941b9143475",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
