{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "id": "FANBas6ESkyJ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 4, 4) (1, 4, 4)\n",
      "True\n",
      "(1, 4, 8)\n",
      "(1, 4, 2, 2, 2)\n",
      "(2, 1, 4, 2, 2)\n",
      "True\n",
      "(1, 4, 2, 2) (1, 4, 2, 2)\n",
      "(1, 4, 2, 4)\n",
      "(1, 4, 2, 2)\n",
      "(1, 4, 4) (1, 4, 0)\n",
      "True\n",
      "(1, 4, 4)\n",
      "(1, 4, 2)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from keras.layers import Dense\n",
    "\n",
    "b, w, h, ch, dim, l = 1, 2, 2, 2, 2, 4\n",
    "heads = 2\n",
    "\n",
    "img = tf.range(0, b * w * h * ch, dtype=float)\n",
    "img = tf.reshape(img, (b, w * h, ch))\n",
    "# so now the image is flattened to a sequence of tokens and every value of the array is unique\n",
    "k = tf.ones((dim, l))\n",
    "\n",
    "proj = tf.ones((heads * dim, dim))\n",
    "\n",
    "qkv = tf.ones((ch, dim * 2 * heads))\n",
    "# here we do the first einsum which should convert every token to a new dimension.  This should be symmetric in the heads dimension.  If we slice this array along the channel dimension it should be symmetric\n",
    "inter = tf.einsum('bnc,cd->bnd', img, qkv)\n",
    "# this is where q and v should come from\n",
    "print(inter[:, :, :dim*heads].shape, inter[:, :, dim*heads:].shape)\n",
    "print(np.array_equal(inter[:, :, :dim*heads], inter[:, :, dim*heads:]))\n",
    "print(inter.shape)\n",
    "qv = tf.reshape(inter, (b, w * h, heads, 2, dim))\n",
    "#TODO: verify this reshape is working as expected\n",
    "print(qv.shape)\n",
    "qv = tf.transpose(qv, perm=[3, 0, 1, 2, 4])\n",
    "print(qv.shape)\n",
    "\n",
    "q, v = qv[0], qv[1]\n",
    "print(np.array_equal(q, v))\n",
    "print(q.shape, v.shape)\n",
    "\n",
    "attn = tf.einsum('bnik,kr->bnir', q, k)\n",
    "print(attn.shape)\n",
    "\n",
    "res = tf.einsum('bnir,bnik->brik', attn, v)\n",
    "print(res.shape)\n",
    "\n",
    "#TODO: verify this reshape is working as expected\n",
    "out = tf.reshape(res, (b, l, heads * dim))\n",
    "print(out[:, :, :dim*heads].shape, out[:, :, dim*heads:].shape)\n",
    "print(np.array_equal(out[:, :, :dim], out[:, :, dim:]))\n",
    "print(out.shape)\n",
    "out = tf.einsum('bnc,cd->bnd', out, proj)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "4dJWwHytppVD"
   },
   "outputs": [],
   "source": [
    "r = tf.range(0, 8)\n",
    "r = tf.reshape(r, (2, 2, 2))\n",
    "# so, the last axes are filled like digits from the back\n",
    "# when the last axis reaches max index the next axis is iterated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5WZrBbWdqy9i",
    "outputId": "f9269549-44f2-4bf3-a037-a60610a63a04"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 2), dtype=int32, numpy=\n",
       "array([[0, 1],\n",
       "       [2, 3]], dtype=int32)>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JiAUGJZQq4sG",
    "outputId": "4b00317a-b12d-4da7-a7c6-849ed7f47fde"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([0 1], shape=(2,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "print(r[0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "oGGeLxoXuqF6"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dropout\n",
    "\n",
    "class LunchboxMHSA(tf.keras.layers.Layer):\n",
    "    def __init__(self,\n",
    "                 dim,\n",
    "                 num_heads,\n",
    "                 lunchbox_dim,\n",
    "                 packed=False,\n",
    "                 qkv_bias=True,\n",
    "                 qk_scale=None,\n",
    "                 proj_drop=0.,\n",
    "                 prefix=''):\n",
    "        \"\"\"\n",
    "        Dot product self-attention where the K table is a matrix of learnable\n",
    "        parameters.  We use the 'Lunchbox' metaphor within this layer where the \n",
    "        lunchbox is K, and the savory lunchtime treats are the columns of K.\n",
    "        Packing is intended to refer to a stage of transfer learning wherein the\n",
    "        weights that form K are learned from an external task.  The lunchbox\n",
    "        is considered 'packed' after learning on the external task, and 'unpacked'\n",
    "        during training on the external task.  Call .pack() to freeze K, call\n",
    "        .unpack() to unfreeze them.\n",
    "\n",
    "        :param dim: embedding dimension for k, q, v\n",
    "        :param num_heads: number of attention heads\n",
    "        :param packed: whether or not the lunchbox should be considered packed (True -> k is not trainable)\n",
    "        :param qkv_bias: whether or not to use bias in the initial projection to dim\n",
    "        :param qk scale: scale for rescaling as show in https://arxiv.org/abs/1706.03762, defaults to dim ** -0.5\n",
    "        :param proj_drop: dropout rate for output\n",
    "        :param prefix: name of this layer\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.num_heads = num_heads\n",
    "        self.lunchbox_dim = lunchbox_dim\n",
    "        self.qkv_bias = qkv_bias\n",
    "\n",
    "        self.scale = qk_scale or max(dim, lunchbox_dim) ** -0.5\n",
    "        self.prefix = prefix\n",
    "\n",
    "        self.qkv = None\n",
    "\n",
    "        self.proj = None\n",
    "        self.proj_drop = Dropout(proj_drop)\n",
    "        self.k = None\n",
    "        self.built = False\n",
    "        self.packed = packed\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.k = self.add_weight(f'{self.prefix}/attn/lunchbox',\n",
    "                                 shape=(self.dim, self.lunchbox_dim),\n",
    "                                 initializer=tf.initializers.GlorotUniform(), \n",
    "                                 trainable=self.packed)\n",
    "        \n",
    "        self.proj = self.add_weight(f'{self.prefix}/attn/proj',\n",
    "                                 shape=(self.num_heads * self.dim, self.dim),\n",
    "                                 initializer=tf.initializers.GlorotUniform(), \n",
    "                                 trainable=True)\n",
    "\n",
    "        self.qkv = self.add_weight(f'{self.prefix}/attn/qkv',\n",
    "                          shape=(input_shape[-1], self.dim * 2 * self.num_heads),\n",
    "                          initializer=tf.initializers.GlorotUniform(), \n",
    "                          trainable=True)\n",
    "\n",
    "        self.built = True\n",
    "\n",
    "\n",
    "    def pack(self):\n",
    "      # make the table untrainable\n",
    "      self.k = tf.Variable(self.k, trainable=False)\n",
    "\n",
    "    def unpack(self):\n",
    "      # make the table trainable\n",
    "      self.k = tf.Variable(self.k, trainable=True)\n",
    "\n",
    "    def call(self, x):\n",
    "\n",
    "        B_, N, C = x.get_shape().as_list()\n",
    "\n",
    "        # x = tf.reshape(x, (B_, N, C))\n",
    "        # (b, n, ch), (ch, dim) -> (b, n, dim * 2 * h)\n",
    "        x = tf.einsum('bnc,cd->bnd', x, self.qkv)\n",
    "        x = tf.transpose(x, perm=[0, 2, 1])\n",
    "\n",
    "        x = tf.reshape(x, (-1, 2, self.num_heads, self.dim, N))\n",
    "        qv = tf.transpose(x, perm=[1, 0, 4, 2, 3])\n",
    "\n",
    "        q, v = qv[0], qv[1]\n",
    "\n",
    "        attn = tf.einsum('bnik,kr->bnir', q, self.k)\n",
    "        attn = attn * self.scale\n",
    "        tf.nn.softmax(attn, -1)\n",
    "\n",
    "        res = tf.einsum('bnir,bnik->brik', attn, v)\n",
    "\n",
    "        x = tf.reshape(res, (-1, self.lunchbox_dim, self.num_heads * self.dim))\n",
    "\n",
    "        x = self.proj(x)\n",
    "        # (b, n, dim * n_h), (ch, dim) -> (b, n, dim)\n",
    "        x = tf.einsum('bnc,cd->bnd', x, self.proj)\n",
    "\n",
    "        x = self.proj_drop(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def get_config(self):\n",
    "        return {\"k\": self.k.numpy()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "FC5GhaFVu1vO"
   },
   "outputs": [],
   "source": [
    "lb = LunchboxMHSA(2,2,2)\n",
    "lb.build((1, ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9tKRV174wRQM",
    "outputId": "ec8cd2b8-a4ab-4fd8-b7f0-43b02c4e2ba2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable '/attn/proj:0' shape=(4, 2) dtype=float32, numpy=\n",
       " array([[-0.5814152 , -0.8283603 ],\n",
       "        [ 0.12946153,  0.5329387 ],\n",
       "        [-0.13148117, -0.22390294],\n",
       "        [-0.5452256 ,  0.9112253 ]], dtype=float32)>,\n",
       " <tf.Variable '/attn/qkv:0' shape=(1, 8) dtype=float32, numpy=\n",
       " array([[-0.58784664,  0.3195368 , -0.45753336, -0.15700167, -0.6013385 ,\n",
       "          0.49908793, -0.26111907,  0.17951947]], dtype=float32)>]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lb.trainable_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "q-6z_e7RwuAl",
    "outputId": "3fc12b63-d6e6-449e-e16f-a905970f5026"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[-0.5814152 , -0.8283603 ],\n",
       "        [ 0.12946153,  0.5329387 ],\n",
       "        [-0.13148117, -0.22390294],\n",
       "        [-0.5452256 ,  0.9112253 ]], dtype=float32),\n",
       " array([[-0.58784664,  0.3195368 , -0.45753336, -0.15700167, -0.6013385 ,\n",
       "          0.49908793, -0.26111907,  0.17951947]], dtype=float32),\n",
       " array([[ 0.757552  , -1.1105257 ],\n",
       "        [ 0.48122764,  0.08933914]], dtype=float32)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = lb.get_weights()\n",
    "lb.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "id": "eJHRgCw_xGc9"
   },
   "outputs": [],
   "source": [
    "lb.set_weights(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KxEBvZNrDqvH",
    "outputId": "82eaa502-ca4a-424c-edf3-bbe3ab8003ee"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable '/attn/proj:0' shape=(4, 2) dtype=float32, numpy=\n",
       " array([[-0.6378875 , -0.45413303],\n",
       "        [ 0.5230794 ,  0.88192725],\n",
       "        [-0.21150255,  0.99610925],\n",
       "        [ 0.17696261,  0.57753515]], dtype=float32)>,\n",
       " <tf.Variable '/attn/qkv:0' shape=(1, 8) dtype=float32, numpy=\n",
       " array([[ 0.2691133 ,  0.68519545,  0.43147898,  0.2203914 , -0.08316743,\n",
       "          0.05300945, -0.5406209 , -0.5833623 ]], dtype=float32)>]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lb.trainable_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "id": "T3R5FgIaDwjY"
   },
   "outputs": [],
   "source": [
    "lb.unpack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ELAd4l7aDz40",
    "outputId": "abc1e43e-b074-49df-e03c-f964788dbff9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable '/attn/proj:0' shape=(4, 2) dtype=float32, numpy=\n",
       " array([[-0.6378875 , -0.45413303],\n",
       "        [ 0.5230794 ,  0.88192725],\n",
       "        [-0.21150255,  0.99610925],\n",
       "        [ 0.17696261,  0.57753515]], dtype=float32)>,\n",
       " <tf.Variable '/attn/qkv:0' shape=(1, 8) dtype=float32, numpy=\n",
       " array([[ 0.2691133 ,  0.68519545,  0.43147898,  0.2203914 , -0.08316743,\n",
       "          0.05300945, -0.5406209 , -0.5833623 ]], dtype=float32)>,\n",
       " <tf.Variable 'Variable:0' shape=(2, 2) dtype=float32, numpy=\n",
       " array([[-0.9065486 , -1.1240162 ],\n",
       "        [ 1.2227877 ,  0.37457538]], dtype=float32)>]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lb.trainable_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "id": "2-VM6s3SGz3x"
   },
   "outputs": [],
   "source": [
    "lb.pack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2TWPcZI7G0-7",
    "outputId": "6b1a509a-6e6c-4a07-947a-5e4be81fe96d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable '/attn/proj:0' shape=(4, 2) dtype=float32, numpy=\n",
       " array([[-0.6378875 , -0.45413303],\n",
       "        [ 0.5230794 ,  0.88192725],\n",
       "        [-0.21150255,  0.99610925],\n",
       "        [ 0.17696261,  0.57753515]], dtype=float32)>,\n",
       " <tf.Variable '/attn/qkv:0' shape=(1, 8) dtype=float32, numpy=\n",
       " array([[ 0.2691133 ,  0.68519545,  0.43147898,  0.2203914 , -0.08316743,\n",
       "          0.05300945, -0.5406209 , -0.5833623 ]], dtype=float32)>]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lb.trainable_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "id": "UB6DWghDG2KN"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dropout, Dense\n",
    "\n",
    "class PCALunchboxMHSA(tf.keras.layers.Layer):\n",
    "    def __init__(self,\n",
    "                 dim,\n",
    "                 num_heads,\n",
    "                 lunchbox_dim,\n",
    "                 packed=False,\n",
    "                 qkv_bias=True,\n",
    "                 qk_scale=None,\n",
    "                 proj_drop=0.,\n",
    "                 prefix=''):\n",
    "        \"\"\"\n",
    "        Dot product self-attention where the K table is a matrix of learnable\n",
    "        parameters.  We use the 'Lunchbox' metaphor within this layer where the \n",
    "        lunchbox is K, and the savory lunchtime treats are the columns of K.\n",
    "        Packing is intended to refer to a stage of transfer learning wherein the\n",
    "        weights that form K are learned from an external task.  The lunchbox\n",
    "        is considered 'packed' after learning on the external task, and 'unpacked'\n",
    "        during training on the external task.  Call .pack() to freeze K, call\n",
    "        .unpack() to unfreeze them.\n",
    "\n",
    "        :param dim: embedding dimension for k, q, v\n",
    "        :param num_heads: number of attention heads\n",
    "        :param packed: whether or not the lunchbox should be considered packed (True -> k is not trainable)\n",
    "        :param qkv_bias: whether or not to use bias in the initial projection to dim\n",
    "        :param qk scale: scale for rescaling as show in https://arxiv.org/abs/1706.03762, defaults to dim ** -0.5\n",
    "        :param proj_drop: dropout rate for output\n",
    "        :param prefix: name of this layer\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.num_heads = num_heads\n",
    "        self.lunchbox_dim = lunchbox_dim\n",
    "        self.qkv_bias = qkv_bias\n",
    "\n",
    "        self.scale = qk_scale or max(dim, lunchbox_dim) ** -0.5\n",
    "        self.prefix = prefix\n",
    "\n",
    "        self.qkv = None\n",
    "\n",
    "        self.proj = None\n",
    "        self.proj_drop = Dropout(proj_drop)\n",
    "        self.k = None\n",
    "        self.built = False\n",
    "        self.packed = packed\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.k = self.add_weight(f'{self.prefix}/attn/lunchbox',\n",
    "                                 shape=(self.dim, self.lunchbox_dim),\n",
    "                                 initializer=tf.initializers.GlorotUniform(), \n",
    "                                 trainable=self.packed)\n",
    "        \n",
    "        self.proj = self.add_weight(f'{self.prefix}/attn/proj',\n",
    "                                 shape=(self.num_heads * self.dim, self.dim),\n",
    "                                 initializer=tf.initializers.GlorotUniform(), \n",
    "                                 trainable=True)\n",
    "\n",
    "        self.qkv = self.add_weight(f'{self.prefix}/attn/qkv',\n",
    "                          shape=(input_shape[-1], self.dim * 2 * self.num_heads),\n",
    "                          initializer=tf.initializers.GlorotUniform(), \n",
    "                          trainable=True)\n",
    "\n",
    "        self.built = True\n",
    "\n",
    "\n",
    "    def pack(self):\n",
    "      # make the table untrainable\n",
    "      self.k = tf.Variable(self.k, trainable=False)\n",
    "\n",
    "    def unpack(self):\n",
    "      # make the table trainable\n",
    "      self.k = tf.Variable(self.k, trainable=True)\n",
    "\n",
    "    def call(self, x):\n",
    "\n",
    "        B_, N, C = x.get_shape().as_list()\n",
    "\n",
    "        # x = tf.reshape(x, (B_, N, C))\n",
    "        # (b, n, ch), (ch, dim) -> (b, n, dim * 2 * h)\n",
    "        x = tf.einsum('bnc,cd->bnd', x, self.qkv)\n",
    "        x = tf.transpose(x, perm=[0, 2, 1])\n",
    "\n",
    "        x = tf.reshape(x, (-1, 2, self.num_heads, self.dim, N))\n",
    "        qv = tf.transpose(x, perm=[1, 0, 2, 3, 4])\n",
    "\n",
    "        q, v = qv[0], qv[1]\n",
    "\n",
    "        attn = tf.einsum('bikj,kr->birk', q, self.k)\n",
    "        attn = attn * self.scale\n",
    "        tf.nn.softmax(attn, -1)\n",
    "\n",
    "        res = tf.einsum('birk,bikn->bink', attn, v)\n",
    "\n",
    "        x = tf.reshape(res, (-1, N, self.num_heads * self.dim))\n",
    "\n",
    "        x = self.proj(x)\n",
    "        # (b, n, dim * n_h), (ch, dim) -> (b, n, dim)\n",
    "        x = tf.einsum('bnc,cd->bnd', x, self.proj)\n",
    "\n",
    "        x = self.proj_drop(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def get_config(self):\n",
    "        return {\"k\": self.k.numpy()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'support'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msupport\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Config, Experiment\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtrainable\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvit\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m build_focal_LAXNet, build_basic_lunchbox\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtrainable\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcnn\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m build_basic_convnextv2, build_basic_cnn\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'support'"
     ]
    }
   ],
   "source": [
    "from support.util import Config, Experiment\n",
    "\n",
    "from trainable.models.vit import build_focal_LAXNet, build_basic_lunchbox\n",
    "from trainable.models.cnn import build_basic_convnextv2, build_basic_cnn\n",
    "\n",
    "\n",
    "from data.datasets.image_classification import deep_weeds, cats_dogs, dot_dataset, citrus_leaves\n",
    "from optimization.data_augmentation.msda import mixup_dset, blended_dset\n",
    "from optimization.data_augmentation.ssda import add_gaussian_noise_dset, custom_rand_augment_dset, foff_dset\n",
    "\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "from optimization.callbacks import EarlyStoppingDifference\n",
    "\n",
    "from optimization.training_loops.supervised import keras_supervised\n",
    "from optimization.schedules import bleed_out\n",
    "\"\"\"\n",
    "hardware_params must include:\n",
    "\n",
    "    'n_gpu': uint\n",
    "    'n_cpu': uint\n",
    "    'node': str\n",
    "    'partition': str\n",
    "    'time': str (we will just write this to the file)\n",
    "    'memory': uint\n",
    "    'distributed': bool\n",
    "\"\"\"\n",
    "hardware_params = {\n",
    "    'name': 'hparam',\n",
    "    'n_gpu': 4,\n",
    "    'n_cpu': 16,\n",
    "    'partition': 'ai2es',\n",
    "    'nodelist': ['c733'],\n",
    "    'time': '96:00:00',\n",
    "    'memory': 16384,\n",
    "    # The %04a is translated into a 4-digit number that encodes the SLURM_ARRAY_TASK_ID\n",
    "    'stdout_path': '/scratch/jroth/supercomputer/text_outputs/exp%01a_stdout_%A.txt',\n",
    "    'stderr_path': '/scratch/jroth/supercomputer/text_outputs/exp%01a_stderr_%A.txt',\n",
    "    'email': 'jay.c.rothenberger@ou.edu',\n",
    "    'dir': '/scratch/jroth/AI2ES-DL/',\n",
    "    'array': '[1]',\n",
    "    'results_dir': 'results'\n",
    "}\n",
    "\"\"\"\n",
    "network_params must include:\n",
    "    \n",
    "    'network_fn': network building function\n",
    "    'network_args': arguments to pass to network building function\n",
    "        network_args must include:\n",
    "            'lrate': float\n",
    "    'hyperband': bool\n",
    "\"\"\"\n",
    "image_size = (128, 128, 3)\n",
    "\n",
    "network_params = {\n",
    "    'network_fn': build_basic_lunchbox,\n",
    "    'network_args': {\n",
    "        'lrate': 5e-4,\n",
    "        'n_classes': 2,\n",
    "        'iterations': 6,\n",
    "        'conv_filters': '[32, 48, 64, 96]',\n",
    "        'conv_size': '[3]',\n",
    "        'dense_layers': '[16]',\n",
    "        'learning_rate': [5e-4],\n",
    "        'image_size': image_size,\n",
    "        'l1': None,\n",
    "        'l2': None,\n",
    "        'alpha': [1, 2**(-10)],\n",
    "        'beta': [2**(-7)],\n",
    "        'noise_level': 0.005,\n",
    "        'depth': 4,\n",
    "    },\n",
    "    'hyperband': False\n",
    "}\n",
    "\n",
    "\"\"\"\n",
    "experiment_params must include:\n",
    "    \n",
    "    'seed': random seed for computation\n",
    "    'steps_per_epoch': uint\n",
    "    'validation_steps': uint\n",
    "    'patience': uint\n",
    "    'min_delta': float\n",
    "    'epochs': uint\n",
    "    'nogo': bool\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "experiment_params = {\n",
    "    'seed': 42,\n",
    "    'steps_per_epoch': 512,\n",
    "    'validation_steps': 256,\n",
    "    'patience': 3,\n",
    "    'min_delta': 0.0,\n",
    "    'epochs': 64,\n",
    "    'nogo': False,\n",
    "}\n",
    "\"\"\"\n",
    "dataset_params must include:\n",
    "    'dset_fn': dataset loading function\n",
    "    'dset_args': arguments for dataset loading function\n",
    "    'cache': str or bool\n",
    "    'batch': uint\n",
    "    'prefetch': uint\n",
    "    'shuffle': bool\n",
    "    'augs': iterable of data augmentation functions\n",
    "\"\"\"\n",
    "dataset_params = {\n",
    "    'dset_fn': cats_dogs,\n",
    "    'dset_args': {\n",
    "        'image_size': image_size[:-1],\n",
    "        'path': '../data/'\n",
    "    },\n",
    "    'cache': False,\n",
    "    'cache_to_lscratch': False,\n",
    "    'batch': 64,\n",
    "    'prefetch': 4,\n",
    "    'shuffle': True,\n",
    "    'augs': []\n",
    "}\n",
    "\n",
    "optimization_params = {\n",
    "    'callbacks': [\n",
    "        # EarlyStoppingDifference(patience=experiment_params['patience'],\n",
    "        #                        restore_best_weights=True,\n",
    "        #                        min_delta=experiment_params['min_delta'],\n",
    "        #                        metric_0='val_clam_categorical_accuracy',\n",
    "        #                        metric_1='val_clam_1_categorical_accuracy',\n",
    "        #                        n_classes=2),\n",
    "\n",
    "        LearningRateScheduler(bleed_out(network_params['network_args']['learning_rate'])),\n",
    "        # LossWeightScheduler(loss_weight_schedule)\n",
    "    ],\n",
    "    'training_loop': keras_supervised\n",
    "}\n",
    "\n",
    "config = Config(hardware_params, network_params, dataset_params, experiment_params, optimization_params)\n",
    "\n",
    "exp = Experiment(config)\n",
    "exp.run_array(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
