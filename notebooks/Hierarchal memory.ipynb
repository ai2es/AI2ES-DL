{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a03de8-6c4f-40dd-b448-0ef602172675",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-01 20:39:59.874687: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/home/fagg/miniconda3/envs/tf_bleeding5/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of Hyperparameter Grid: 2\n",
      "{\n",
      "\tname: hparam\n",
      "\tn_gpu: 4\n",
      "\tn_cpu: 16\n",
      "\tpartition: ai2es\n",
      "\tnodelist: ['c732']\n",
      "\ttime: 96:00:00\n",
      "\tmemory: 16384\n",
      "\tstdout_path: /scratch/jroth/supercomputer/text_outputs/exp%01a_stdout_%A.txt\n",
      "\tstderr_path: /scratch/jroth/supercomputer/text_outputs/exp%01a_stderr_%A.txt\n",
      "\temail: jay.c.rothenberger@ou.edu\n",
      "\tdir: /scratch/jroth/AI2ES-DL/\n",
      "\tarray: [1]\n",
      "\tresults_dir: results\n",
      "\t}\n",
      "{\n",
      "\tseed: 42\n",
      "\tsteps_per_epoch: 512\n",
      "\tvalidation_steps: 256\n",
      "\tpatience: 3\n",
      "\tmin_delta: 0.0\n",
      "\tepochs: 5\n",
      "\tnogo: False\n",
      "\t}\n",
      "{\n",
      "\tnetwork_fn: <function lunchbox_packerv2 at 0x2ac2e3efcee0>\n",
      "\tnetwork_args: {\n",
      "\t\thyperband: False\n",
      "\t\tnoise_level: 0.005\n",
      "\t\tl2: None\n",
      "\t\tdense_layers: [16]\n",
      "\t\tdepth: 3\n",
      "\t\tconv_filters: 24\n",
      "\t\tlrate: 0.0005\n",
      "\t\tconv_size: [3]\n",
      "\t\timage_size: (32, 32, 3)\n",
      "\t\titerations: 6\n",
      "\t\talpha: 1\n",
      "\t\tbeta: 0.0078125\n",
      "\t\tlearning_rate: 0.0005\n",
      "\t\tl1: None\n",
      "\t\tn_classes: 2\n",
      "\t\t}\n",
      "\thyperband: False\n",
      "\t}\n",
      "{\n",
      "\tdset_fn: <function cifar10 at 0x2ac2e4e1bbe0>\n",
      "\tdset_args: {\n",
      "\t\timage_size: (32, 32)\n",
      "\t\tpath: ../data/\n",
      "\t\t}\n",
      "\tcache: False\n",
      "\tcache_to_lscratch: False\n",
      "\tbatch: 256\n",
      "\tprefetch: 4\n",
      "\tshuffle: True\n",
      "\taugs: []\n",
      "\t}\n",
      "4 [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:1', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:2', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:3', device_type='GPU')]\n",
      "We have 4 GPUs\n",
      "\n",
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1', '/job:localhost/replica:0/task:0/device:GPU:2', '/job:localhost/replica:0/task:0/device:GPU:3')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1', '/job:localhost/replica:0/task:0/device:GPU:2', '/job:localhost/replica:0/task:0/device:GPU:3')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512 None\n",
      "Model: \"lunchbox_ae\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 32, 32, 3)]       0         \n",
      "                                                                 \n",
      " reshape (Reshape)           (None, 1024, 3)           0         \n",
      "                                                                 \n",
      " v_lunchbox_mhsa (VLunchboxM  (None, 1024, 24)         59520     \n",
      " HSA)                                                            \n",
      "                                                                 \n",
      " q_lunchbox_mhsa (QLunchboxM  (None, 512, 24)          26112     \n",
      " HSA)                                                            \n",
      "                                                                 \n",
      " layer_normalization (LayerN  (None, 512, 24)          48        \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " v_lunchbox_mhsa_1 (VLunchbo  (None, 512, 24)          43008     \n",
      " xMHSA)                                                          \n",
      "                                                                 \n",
      " q_lunchbox_mhsa_1 (QLunchbo  (None, 256, 24)          19968     \n",
      " xMHSA)                                                          \n",
      "                                                                 \n",
      " layer_normalization_1 (Laye  (None, 256, 24)          48        \n",
      " rNormalization)                                                 \n",
      "                                                                 \n",
      " v_lunchbox_mhsa_2 (VLunchbo  (None, 256, 24)          30720     \n",
      " xMHSA)                                                          \n",
      "                                                                 \n",
      " q_lunchbox_mhsa_2 (QLunchbo  (None, 128, 24)          16896     \n",
      " xMHSA)                                                          \n",
      "                                                                 \n",
      " layer_normalization_2 (Laye  (None, 128, 24)          48        \n",
      " rNormalization)                                                 \n",
      "                                                                 \n",
      " v_lunchbox_mhsa_3 (VLunchbo  (None, 128, 24)          24576     \n",
      " xMHSA)                                                          \n",
      "                                                                 \n",
      " layer_normalization_3 (Laye  (None, 128, 24)          48        \n",
      " rNormalization)                                                 \n",
      "                                                                 \n",
      " v_lunchbox_mhsa_4 (VLunchbo  (None, 128, 24)          24576     \n",
      " xMHSA)                                                          \n",
      "                                                                 \n",
      " q_lunchbox_mhsa_3 (QLunchbo  (None, 256, 24)          19968     \n",
      " xMHSA)                                                          \n",
      "                                                                 \n",
      " layer_normalization_4 (Laye  (None, 256, 24)          48        \n",
      " rNormalization)                                                 \n",
      "                                                                 \n",
      " v_lunchbox_mhsa_5 (VLunchbo  (None, 256, 24)          30720     \n",
      " xMHSA)                                                          \n",
      "                                                                 \n",
      " q_lunchbox_mhsa_4 (QLunchbo  (None, 512, 24)          26112     \n",
      " xMHSA)                                                          \n",
      "                                                                 \n",
      " layer_normalization_5 (Laye  (None, 512, 24)          48        \n",
      " rNormalization)                                                 \n",
      "                                                                 \n",
      " v_lunchbox_mhsa_6 (VLunchbo  (None, 512, 24)          43008     \n",
      " xMHSA)                                                          \n",
      "                                                                 \n",
      " q_lunchbox_mhsa_5 (QLunchbo  (None, 1024, 24)         38400     \n",
      " xMHSA)                                                          \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1024, 3)           75        \n",
      "                                                                 \n",
      " reshape_1 (Reshape)         (None, 32, 32, 3)         0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 403,947\n",
      "Trainable params: 403,947\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-01 20:41:21.266508: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Found an unshardable source dataset: name: \"TensorSliceDataset/_1\"\n",
      "op: \"TensorSliceDataset\"\n",
      "input: \"Placeholder/_0\"\n",
      "attr {\n",
      "  key: \"Toutput_types\"\n",
      "  value {\n",
      "    list {\n",
      "      type: DT_UINT8\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"_cardinality\"\n",
      "  value {\n",
      "    i: 50000\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"is_files\"\n",
      "  value {\n",
      "    b: false\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"metadata\"\n",
      "  value {\n",
      "    s: \"\\n\\025TensorSliceDataset:29\"\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_shapes\"\n",
      "  value {\n",
      "    list {\n",
      "      shape {\n",
      "        dim {\n",
      "          size: 32\n",
      "        }\n",
      "        dim {\n",
      "          size: 32\n",
      "        }\n",
      "        dim {\n",
      "          size: 3\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"replicate_on_split\"\n",
      "  value {\n",
      "    b: false\n",
      "  }\n",
      "}\n",
      "experimental_type {\n",
      "  type_id: TFT_PRODUCT\n",
      "  args {\n",
      "    type_id: TFT_DATASET\n",
      "    args {\n",
      "      type_id: TFT_PRODUCT\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_UINT8\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "INFO:tensorflow:batch_all_reduce: 53 all-reduces with algorithm = nccl, num_packs = 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:batch_all_reduce: 53 all-reduces with algorithm = nccl, num_packs = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:batch_all_reduce: 53 all-reduces with algorithm = nccl, num_packs = 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:batch_all_reduce: 53 all-reduces with algorithm = nccl, num_packs = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512/512 [==============================] - ETA: 0s - loss: 0.2115"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-01 20:44:29.954298: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Found an unshardable source dataset: name: \"TensorSliceDataset/_1\"\n",
      "op: \"TensorSliceDataset\"\n",
      "input: \"Placeholder/_0\"\n",
      "attr {\n",
      "  key: \"Toutput_types\"\n",
      "  value {\n",
      "    list {\n",
      "      type: DT_UINT8\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"_cardinality\"\n",
      "  value {\n",
      "    i: 10000\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"is_files\"\n",
      "  value {\n",
      "    b: false\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"metadata\"\n",
      "  value {\n",
      "    s: \"\\n\\025TensorSliceDataset:36\"\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_shapes\"\n",
      "  value {\n",
      "    list {\n",
      "      shape {\n",
      "        dim {\n",
      "          size: 32\n",
      "        }\n",
      "        dim {\n",
      "          size: 32\n",
      "        }\n",
      "        dim {\n",
      "          size: 3\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"replicate_on_split\"\n",
      "  value {\n",
      "    b: false\n",
      "  }\n",
      "}\n",
      "experimental_type {\n",
      "  type_id: TFT_PRODUCT\n",
      "  args {\n",
      "    type_id: TFT_DATASET\n",
      "    args {\n",
      "      type_id: TFT_PRODUCT\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_UINT8\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512/512 [==============================] - 202s 345ms/step - loss: 0.2115 - val_loss: 0.1757 - lr: 5.0000e-04\n",
      "Epoch 2/5\n",
      "512/512 [==============================] - 170s 332ms/step - loss: 0.1752 - val_loss: 0.1756 - lr: 3.5697e-04\n",
      "Epoch 3/5\n",
      " 85/512 [===>..........................] - ETA: 2:08 - loss: 0.1747"
     ]
    }
   ],
   "source": [
    "from support.util import Config, Experiment\n",
    "\n",
    "from trainable.models.vit import build_focal_LAXNet, build_basic_lunchbox\n",
    "from trainable.models.cnn import build_basic_convnextv2, build_basic_cnn\n",
    "from trainable.models.ae import lunchbox_packerv2\n",
    "\n",
    "from data.datasets.image_classification import deep_weeds, cats_dogs, dot_dataset, citrus_leaves\n",
    "from data.datasets.image_to_image import cifar10\n",
    "from optimization.data_augmentation.msda import mixup_dset, blended_dset\n",
    "from optimization.data_augmentation.ssda import add_gaussian_noise_dset, custom_rand_augment_dset, foff_dset\n",
    "\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "from optimization.callbacks import EarlyStoppingDifference\n",
    "\n",
    "from optimization.training_loops.supervised import keras_supervised\n",
    "from optimization.schedules import bleed_out\n",
    "\"\"\"\n",
    "hardware_params must include:\n",
    "\n",
    "    'n_gpu': uint\n",
    "    'n_cpu': uint\n",
    "    'node': str\n",
    "    'partition': str\n",
    "    'time': str (we will just write this to the file)\n",
    "    'memory': uint\n",
    "    'distributed': bool\n",
    "\"\"\"\n",
    "hardware_params = {\n",
    "    'name': 'hparam',\n",
    "    'n_gpu': 4,\n",
    "    'n_cpu': 16,\n",
    "    'partition': 'ai2es',\n",
    "    'nodelist': ['c732'],\n",
    "    'time': '96:00:00',\n",
    "    'memory': 16384,\n",
    "    # The %04a is translated into a 4-digit number that encodes the SLURM_ARRAY_TASK_ID\n",
    "    'stdout_path': '/scratch/jroth/supercomputer/text_outputs/exp%01a_stdout_%A.txt',\n",
    "    'stderr_path': '/scratch/jroth/supercomputer/text_outputs/exp%01a_stderr_%A.txt',\n",
    "    'email': 'jay.c.rothenberger@ou.edu',\n",
    "    'dir': '/scratch/jroth/AI2ES-DL/',\n",
    "    'array': '[1]',\n",
    "    'results_dir': 'results'\n",
    "}\n",
    "\"\"\"\n",
    "network_params must include:\n",
    "    \n",
    "    'network_fn': network building function\n",
    "    'network_args': arguments to pass to network building function\n",
    "        network_args must include:\n",
    "            'lrate': float\n",
    "    'hyperband': bool\n",
    "\"\"\"\n",
    "image_size = (32, 32, 3)\n",
    "\n",
    "network_params = {\n",
    "    'network_fn': lunchbox_packerv2,\n",
    "    'network_args': {\n",
    "        'lrate': 5e-4,\n",
    "        'n_classes': 2,\n",
    "        'iterations': 6,\n",
    "        'conv_filters': 24,\n",
    "        'conv_size': '[3]',\n",
    "        'dense_layers': '[16]',\n",
    "        'learning_rate': [5e-4],\n",
    "        'image_size': image_size,\n",
    "        'l1': None,\n",
    "        'l2': None,\n",
    "        'alpha': [1, 2**(-10)],\n",
    "        'beta': [2**(-7)],\n",
    "        'noise_level': 0.005,\n",
    "        'depth': 3,\n",
    "    },\n",
    "    'hyperband': False\n",
    "}\n",
    "\n",
    "\"\"\"\n",
    "experiment_params must include:\n",
    "    \n",
    "    'seed': random seed for computation\n",
    "    'steps_per_epoch': uint\n",
    "    'validation_steps': uint\n",
    "    'patience': uint\n",
    "    'min_delta': float\n",
    "    'epochs': uint\n",
    "    'nogo': bool\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "experiment_params = {\n",
    "    'seed': 42,\n",
    "    'steps_per_epoch': 512,\n",
    "    'validation_steps': 256,\n",
    "    'patience': 3,\n",
    "    'min_delta': 0.0,\n",
    "    'epochs': 5,\n",
    "    'nogo': False,\n",
    "}\n",
    "\"\"\"\n",
    "dataset_params must include:\n",
    "    'dset_fn': dataset loading function\n",
    "    'dset_args': arguments for dataset loading function\n",
    "    'cache': str or bool\n",
    "    'batch': uint\n",
    "    'prefetch': uint\n",
    "    'shuffle': bool\n",
    "    'augs': iterable of data augmentation functions\n",
    "\"\"\"\n",
    "dataset_params = {\n",
    "    'dset_fn': cifar10,\n",
    "    'dset_args': {\n",
    "        'image_size': image_size[:-1],\n",
    "        'path': '../data/'\n",
    "    },\n",
    "    'cache': False,\n",
    "    'cache_to_lscratch': False,\n",
    "    'batch': 256,\n",
    "    'prefetch': 4,\n",
    "    'shuffle': True,\n",
    "    'augs': []\n",
    "}\n",
    "\n",
    "optimization_params = {\n",
    "    'callbacks': [\n",
    "        # EarlyStoppingDifference(patience=experiment_params['patience'],\n",
    "        #                        restore_best_weights=True,\n",
    "        #                        min_delta=experiment_params['min_delta'],\n",
    "        #                        metric_0='val_clam_categorical_accuracy',\n",
    "        #                        metric_1='val_clam_1_categorical_accuracy',\n",
    "        #                        n_classes=2),\n",
    "\n",
    "        LearningRateScheduler(bleed_out(network_params['network_args']['learning_rate'])),\n",
    "        # LossWeightScheduler(loss_weight_schedule)\n",
    "    ],\n",
    "    'training_loop': keras_supervised\n",
    "}\n",
    "\n",
    "config = Config(hardware_params, network_params, dataset_params, experiment_params, optimization_params)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    exp = Experiment(config)\n",
    "\n",
    "    # print(exp.params)\n",
    "    exp.run_array(0)\n",
    "\n",
    "    # exp.enqueue()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f31fc61e-2c7d-4bad-bc08-71193ce707ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-01 17:34:31.696945: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/home/fagg/miniconda3/envs/tf_bleeding5/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------------------------------------------------------\n",
      "Experimental Results Summary (Index: 0)\n",
      "------------------------------------------------------------\n",
      "Dataset Params: {\n",
      "\tdset_fn: <function cifar10 at 0x2b7489510280>\n",
      "\tdset_args: {\n",
      "\t\timage_size: (32, 32)\n",
      "\t\tpath: ../data/\n",
      "\t\t}\n",
      "\tcache: False\n",
      "\tcache_to_lscratch: False\n",
      "\tbatch: 256\n",
      "\tprefetch: 4\n",
      "\tshuffle: True\n",
      "\taugs: []\n",
      "\t}\n",
      "\n",
      "Network Params:  {\n",
      "\tnetwork_fn: <function lunchbox_packer at 0x2b74895100d0>\n",
      "\tnetwork_args: {\n",
      "\t\thyperband: False\n",
      "\t\timage_size: (32, 32, 3)\n",
      "\t\tl2: None\n",
      "\t\tdepth: 3\n",
      "\t\tlrate: 0.0005\n",
      "\t\tconv_size: [3]\n",
      "\t\talpha: 1\n",
      "\t\tconv_filters: 24\n",
      "\t\tlearning_rate: 0.0005\n",
      "\t\tn_classes: 2\n",
      "\t\tl1: None\n",
      "\t\tnoise_level: 0.005\n",
      "\t\tdense_layers: [16]\n",
      "\t\titerations: 6\n",
      "\t\tbeta: 0.0078125\n",
      "\t\t}\n",
      "\thyperband: False\n",
      "\t}\n",
      "------------------------------------------------------------\n",
      "Experiment Parameters: {\n",
      "\tseed: 42\n",
      "\tsteps_per_epoch: 512\n",
      "\tvalidation_steps: 256\n",
      "\tpatience: 3\n",
      "\tmin_delta: 0.0\n",
      "\tepochs: 5\n",
      "\tnogo: False\n",
      "\t}\n",
      "\n",
      "Experiment Runtime: 801.9894886016846s\n",
      "\n",
      "Epochs Run / Average Time: 5 / 160.3978977203369s\n",
      "\n",
      "Performance At Patience: {\n",
      "\tloss: 0.16602469980716705\n",
      "\tval_loss: 0.16576388478279114\n",
      "\tlr: 0.0003569681430235505\n",
      "\t}\n",
      "------------------------------------------------------------\n",
      "Runtime Parameters: None\n",
      "------------------------------------------------------------\n",
      "        \n",
      "(1, 32, 32, 3) (1, 32, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "from support.util import load_most_recent_results\n",
    "\n",
    "result_path = '../results/'\n",
    "\n",
    "results = load_most_recent_results(result_path, 1)[0]\n",
    "\n",
    "results.summary()\n",
    "results.config.dataset_params['dset_args']['path'] = '../data/'\n",
    "class_names = results.config.dataset_params['dset_fn'](**results.config.dataset_params['dset_args'])['class_names']\n",
    "model_data = results.model_data\n",
    "keras_model = model_data.get_model()\n",
    "test_dset = results.config.dataset_params['dset_fn'](**results.config.dataset_params['dset_args'])['test']\n",
    "test_dset = test_dset.batch(1)\n",
    "\n",
    "for x, y in iter(test_dset):\n",
    "    print(x.shape, y.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0ac209a2-7d27-48a4-bc24-cb0b9c72a1a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_4 (InputLayer)        [(None, 32, 32, 3)]       0         \n",
      "                                                                 \n",
      " layer_normalization (LayerN  (None, 32, 32, 3)        6         \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " reshape (Reshape)           (None, 1024, 3)           0         \n",
      "                                                                 \n",
      " lunchbox_mhsa (LunchboxMHSA  (None, 256, 24)          9024      \n",
      " )                                                               \n",
      "                                                                 \n",
      " reshape_1 (Reshape)         (None, 16, 16, 24)        0         \n",
      "                                                                 \n",
      " layer_normalization_1 (Laye  (None, 16, 16, 24)       48        \n",
      " rNormalization)                                                 \n",
      "                                                                 \n",
      " reshape_2 (Reshape)         (None, 256, 24)           0         \n",
      "                                                                 \n",
      " lunchbox_mhsa_1 (LunchboxMH  (None, 64, 48)           21504     \n",
      " SA)                                                             \n",
      "                                                                 \n",
      " reshape_3 (Reshape)         (None, 8, 8, 48)          0         \n",
      "                                                                 \n",
      " layer_normalization_2 (Laye  (None, 8, 8, 48)         96        \n",
      " rNormalization)                                                 \n",
      "                                                                 \n",
      " reshape_4 (Reshape)         (None, 64, 48)            0         \n",
      "                                                                 \n",
      " lunchbox_mhsa_2 (LunchboxMH  (None, 16, 96)           75264     \n",
      " SA)                                                             \n",
      "                                                                 \n",
      " reshape_5 (Reshape)         (None, 4, 4, 96)          0         \n",
      "                                                                 \n",
      " layer_normalization_3 (Laye  (None, 4, 4, 96)         192       \n",
      " rNormalization)                                                 \n",
      "                                                                 \n",
      " reshape_6 (Reshape)         (None, 16, 96)            0         \n",
      "                                                                 \n",
      " lunchbox_mhsa_3 (LunchboxMH  (None, 16, 96)           112128    \n",
      " SA)                                                             \n",
      "                                                                 \n",
      " reshape_7 (Reshape)         (None, 4, 4, 96)          0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 1536)              0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 10)                15370     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 233,632\n",
      "Trainable params: 221,344\n",
      "Non-trainable params: 12,288\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "encoder_layers = [(i, layer) for i, layer in enumerate(keras_model.layers)][:17]\n",
    "\n",
    "inputs = tf.keras.layers.Input((32, 32, 3))\n",
    "x = inputs\n",
    "for i, layer in encoder_layers[1:]:\n",
    "    if 'lunchbox' in layer.name:\n",
    "        layer.pack()\n",
    "    x = layer(x)\n",
    "\n",
    "x = tf.keras.layers.Flatten()(x)\n",
    "outputs = tf.keras.layers.Dense(10, activation=lambda x: x * tf.nn.relu6(x + 3) / 6)(x)\n",
    "\n",
    "keras_model = tf.keras.models.Model(inputs=[inputs], outputs=[outputs])\n",
    "\n",
    "opt = tf.keras.optimizers.Nadam(learning_rate=1e-4,\n",
    "                                beta_1=0.9, beta_2=0.999,\n",
    "                                epsilon=None, decay=0.99)\n",
    "\n",
    "opt = tf.keras.mixed_precision.LossScaleOptimizer(opt)\n",
    "\n",
    "keras_model.compile(loss='categorical_crossentropy',\n",
    "                    optimizer=opt,\n",
    "                    metrics=['categorical_accuracy'])\n",
    "\n",
    "keras_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4ab455a3-5ad8-46dc-a290-a0640a40455b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 32, 32, 3) (64, 10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-01 17:38:09.978351: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "2023-02-01 17:38:11.044323: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "2023-02-01 17:38:11.058829: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 103s 129ms/step - loss: 8.9818 - categorical_accuracy: 0.0970 - val_loss: 8.9858 - val_categorical_accuracy: 0.0650\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2b79fc82c310>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from data.datasets.image_classification import cifar10 as cifar\n",
    "\n",
    "dset_obc = cifar()\n",
    "\n",
    "train, val = dset_obc['train'].batch(64), dset_obc['val'].batch(64)\n",
    "\n",
    "for x, y in iter(train):\n",
    "    print(x.shape, y.shape)\n",
    "    break\n",
    "\n",
    "keras_model.fit(train, validation_data=val, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9eb5a3c-5ccd-4a2b-ba4f-e2136bbb2751",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
